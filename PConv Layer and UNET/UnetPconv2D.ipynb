{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UnetPconv2D.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrq0eCLRTkhA",
        "colab_type": "text"
      },
      "source": [
        "# ***Importing necessary libraries***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAKvUexO0L22",
        "colab_type": "code",
        "outputId": "6d011ad6-d417-41fb-9f6d-1f04bb3e55d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.engine import InputSpec\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Input, Conv2D, UpSampling2D, Dropout, LeakyReLU, BatchNormalization, Activation, Lambda\n",
        "from keras.layers.merge import Concatenate\n",
        "from keras.applications import VGG16\n",
        "import numpy as np\n",
        "from keras.utils import conv_utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K90bN0esTu8J",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WhXUKgmkq0w",
        "colab_type": "text"
      },
      "source": [
        "# Partial Convolution Layer \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdSNw6-eiu9F",
        "colab_type": "text"
      },
      "source": [
        "Creating a custom keras layer that has trainable weights, \n",
        "by extending the existing Conv2D layer(class) of keras.\n",
        "\n",
        "Resource used : [https://keras.io/layers/writing-your-own-keras-layers/]\n",
        "\n",
        "\n",
        "Mathematical representation of Pconv2D :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFjqy8KjZqSU",
        "colab_type": "text"
      },
      "source": [
        "![](https://ask.qcloudimg.com/http-save/yehe-1407979/xcd0vilkyi.jpeg?imageView2/2/w/1620)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEUXEp-1jpLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Pconv(Conv2D):\n",
        "    \n",
        "    def __init__(self,*args,**kwargs):\n",
        "        '''The following piece of code signifies that the input will be 4-dimensional that is N,C,H,W\n",
        "        N - No of examples\n",
        "        C - No of Channels or filters\n",
        "        H - Height\n",
        "        W - Width\n",
        "        Note that the image and mask will have identical shapes\n",
        "        \n",
        "        the input_spec attribute specifies to the class that\n",
        "        a list is expected as input........'''\n",
        "        super().__init__(*args,**kwargs)\n",
        "        self.input_spec = [InputSpec(ndim=4), InputSpec(ndim=4)]\n",
        "        \n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        ''' As a custom layer with mask is implemented it is necessary to assert,\n",
        "            that the input is a list of [img,mask]. The build function is to define the weights \n",
        "            and biases that would be used in the Pconv layer'''\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "        \n",
        "        if self.data_format == 'channels_first' :\n",
        "            n = 1\n",
        "        else :\n",
        "            n = -1 \n",
        "            \n",
        "        n_channels = input_shape[0][n]\n",
        "        \n",
        "        #Custom made kernel for image convolutions\n",
        "        kernel_shape = self.kernel_size + (n_channels,self.filters)\n",
        "        self.kernel = self.add_weight(name='kernel',\n",
        "                                      shape=kernel_shape,\n",
        "                                      initializer=self.kernel_initializer,\n",
        "                                      regularizer=self.kernel_regularizer,\n",
        "                                      trainable=True,\n",
        "                                      )\n",
        "        \n",
        "        #Custom made kernel for convolutions on mask\n",
        "        self.kernel_mask = K.ones(shape=kernel_shape,name='mask-kernel')\n",
        "        \n",
        "        #this is done to ensure that the output shape is obtained as expected\n",
        "        self.pconv_padding = (\n",
        "            (int((self.kernel_size[0]-1)/2), int((self.kernel_size[0]-1)/2)), \n",
        "            (int((self.kernel_size[0]-1)/2), int((self.kernel_size[0]-1)/2)), \n",
        "        )\n",
        "\n",
        "        # Window size - used for normalization\n",
        "        self.window_size = self.kernel_size[0] * self.kernel_size[1]\n",
        "        \n",
        "        \n",
        "        #bias to add after the custom task in completed\n",
        "        if self.use_bias :\n",
        "            self.bias = self.add_weight(shape=(self.filters,),\n",
        "                                        initializer=self.bias_initializer,\n",
        "                                        name='bias',\n",
        "                                        regularizer=self.bias_regularizer,\n",
        "                                        constraint=self.bias_constraint)\n",
        "            \n",
        "        #super(Pconv, self).build(input_shape)\n",
        "        self.built = True\n",
        "        \n",
        "        \n",
        "    def call(self,inputs) :\n",
        "        ''' This portion includes the actual implementation :\n",
        "            the image is elementally multiplied to the masks\n",
        "            which is fed to the network and later the masks are\n",
        "            updated on the basis of the output generated'''\n",
        "        assert isinstance(inputs, list)\n",
        "        \n",
        "        images = inputs[0]\n",
        "        masks  = inputs[1]\n",
        "        \n",
        "        #padding layer\n",
        "        images = K.spatial_2d_padding(inputs[0], self.pconv_padding, self.data_format)\n",
        "        masks = K.spatial_2d_padding(inputs[1], self.pconv_padding, self.data_format)\n",
        "        \n",
        "        X = images*masks\n",
        "        \n",
        "        out = K.conv2d(X,self.kernel,\n",
        "                       strides=self.strides,\n",
        "                       padding='valid',\n",
        "                       data_format = self.data_format)\n",
        "        \n",
        "        #################################################\n",
        "        def mask_update(masks):\n",
        "            ''' mask update function that updates the mask\n",
        "                after a succesful Partial conv layer\n",
        "                '''\n",
        "            out = K.conv2d(masks, self.kernel_mask,\n",
        "                       strides=self.strides, padding='valid',\n",
        "                       data_format = self.data_format)\n",
        "            out = K.clip(out,0,1)\n",
        "            eps = 1e-6\n",
        "            mask_ratio = self.window_size / (out + eps)\n",
        "            mask_ratio *= out\n",
        "        \n",
        "            return out,mask_ratio\n",
        "        #################################################\n",
        "        \n",
        "        updated_mask,mask_ratio = mask_update(masks)\n",
        "        out = out * mask_ratio\n",
        "        \n",
        "        if self.use_bias :\n",
        "            out = K.bias_add(out, self.bias, data_format=self.data_format)\n",
        "            \n",
        "        if self.activation :\n",
        "            out = self.activation(out)\n",
        "        \n",
        "        return [out, updated_mask]\n",
        "    \n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        ''' in case your layer modifies the shape of its input,\n",
        "        you should specify here the shape transformation logic,\n",
        "        as done in this case'''\n",
        "        assert isinstance(input_shape, list)\n",
        "        \n",
        "        '''if self.data_format == 'channels_last':\n",
        "            shape = input_shape[1:-1]\n",
        "            new_shape = [conv_utils.conv_output_length(\n",
        "                    space[i],\n",
        "                    self.kernel_size[i],\n",
        "                    padding='same',\n",
        "                    stride=self.strides[i],\n",
        "                    dilation=self.dilation_rate[i]) for i in range(len(shape))]\n",
        "            new_shape = (input_shape[0][0],)+tuple(new_shape)+(input_shape[0][-1],)\n",
        "            \n",
        "        if self.data_format == 'channels_first':\n",
        "            shape = input_shape[2:]\n",
        "            new_shape = [conv_utils.conv_output_length(\n",
        "                    space[i],\n",
        "                    self.kernel_size[i],\n",
        "                    padding='same',\n",
        "                    stride=self.strides[i],\n",
        "                    dilation=self.dilation_rate[i]) for i in range(len(shape))]\n",
        "            new_shape = (input_shape[0][0],)+tuple(new_shape)+(input_shape[0][-1],)\n",
        "            \n",
        "        return [new_shape,new_shape]'''\n",
        "        if self.data_format == 'channels_last':\n",
        "            space = input_shape[0][1:-1]\n",
        "            new_space = []\n",
        "            for i in range(len(space)):\n",
        "                new_dim = conv_utils.conv_output_length(\n",
        "                    space[i],\n",
        "                    self.kernel_size[i],\n",
        "                    padding='same',\n",
        "                    stride=self.strides[i],\n",
        "                    dilation=self.dilation_rate[i])\n",
        "                new_space.append(new_dim)\n",
        "            new_shape = (input_shape[0][0],) + tuple(new_space) + (self.filters,)\n",
        "            return [new_shape, new_shape]\n",
        "        if self.data_format == 'channels_first':\n",
        "            space = input_shape[2:]\n",
        "            new_space = []\n",
        "            for i in range(len(space)):\n",
        "                new_dim = conv_utils.conv_output_length(\n",
        "                    space[i],\n",
        "                    self.kernel_size[i],\n",
        "                    padding='same',\n",
        "                    stride=self.strides[i],\n",
        "                    dilation=self.dilation_rate[i])\n",
        "                new_space.append(new_dim)\n",
        "            new_shape = (input_shape[0], self.filters) + tuple(new_space)\n",
        "            return [new_shape, new_shape]\n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRksvHQSTf6C",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlzEjJw9BT8j",
        "colab_type": "text"
      },
      "source": [
        "# U-NET ARCHITECTURE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO5D_RnQBYTE",
        "colab_type": "text"
      },
      "source": [
        "## ![alt text](https://github.com/chefpr7/Image-Inpainting-using-Partial-Convolutional-Layers/blob/master/data/unet-architecture.png?raw=true)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPwWT_dUA_mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PConvUNET(object):\n",
        "    \n",
        "    def __init__(self,h,w,c,data_format='channels_last',weights='imagenet'):\n",
        "        \n",
        "        self.h = h\n",
        "        self.w = w\n",
        "        self.c = c\n",
        "        self.data_format = data_format\n",
        "        self.mean = [0.485, 0.456, 0.406]\n",
        "        self.std = [0.229, 0.224, 0.225]\n",
        "        self.no_of_pixels = h*w*c\n",
        "        self.vgg_layers=[3,6,10]\n",
        "        self.vgg = self.vgg_model(weights)\n",
        "        self.model,self.masks = self.build_pconv_UNet()\n",
        "        self.model_compile(self.model,self.masks)\n",
        "        \n",
        "        \n",
        "    def vgg_model(self,weights):\n",
        "        '''A non-trainable vgg-model trained on imagenet\n",
        "           is used to extract features from higher level\n",
        "           layers namely : pool1,pool2,pool3 which will be\n",
        "           used for calculating perceptual and style losses'''\n",
        "        if weights == 'imagenet' :\n",
        "            vgg = VGG16(include_top=False,weights='imagenet')\n",
        "        else :\n",
        "            vgg = VGG16(weights=None, include_top=False)\n",
        "            vgg.load_weights(weights, by_name=True)\n",
        "        imgs = Input((self.h,self.w,self.c))\n",
        "        processed = Lambda(lambda x: (x-self.mean) / self.std)(imgs)\n",
        "        vgg.outputs = [vgg.layers[i].output for i in self.vgg_layers]\n",
        "        model = Model(inputs=imgs,outputs=vgg(processed))\n",
        "        model.trainable = False\n",
        "        model.compile(loss='mse',optimizer='adam')\n",
        "        return model\n",
        "        \n",
        "    def build_pconv_UNet(self,train_bn=True):\n",
        "        \n",
        "        inputs_img = Input((self.h, self.w, 3), name='inputs_img')\n",
        "        inputs_mask = Input((self.h, self.w, 3), name='inputs_mask')\n",
        "        \n",
        "        def encoder_layer(imgs,masks,filters,kernel_size,bn=True):\n",
        "            I_out , Mask_out = Pconv(filters,kernel_size,strides=2,padding='same')([imgs,masks])\n",
        "            if bn :\n",
        "                I_out = BatchNormalization()(I_out,training=train_bn)\n",
        "            I_out = Activation('relu')(I_out)\n",
        "            \n",
        "            return I_out,Mask_out\n",
        "        \n",
        "        en_img1, en_mask1 = encoder_layer(inputs_img,inputs_mask,64,7,False)\n",
        "        en_img2, en_mask2 = encoder_layer(en_img1,en_mask1,128,5)\n",
        "        en_img3, en_mask3 = encoder_layer(en_img2,en_mask2,256,5)\n",
        "        en_img3, en_mask3 = encoder_layer(en_img2,en_mask2,256,5)\n",
        "        en_img3, en_mask3 = encoder_layer(en_img2,en_mask2,256,5)\n",
        "        en_img4, en_mask4 = encoder_layer(en_img3,en_mask3,512,3)\n",
        "        en_img5, en_mask5 = encoder_layer(en_img4,en_mask4,512,3)\n",
        "        en_img6, en_mask6 = encoder_layer(en_img5,en_mask5,512,3)\n",
        "        en_img7, en_mask7 = encoder_layer(en_img6,en_mask6,512,3)\n",
        "        en_img8, en_mask8 = encoder_layer(en_img7,en_mask7,512,3)\n",
        "        \n",
        "        \n",
        "        def decoder_layer(imgs,masks,en_img,en_mask,filters,kernel_size,bn=True,**kwargs):\n",
        "            I_up    = UpSampling2D(size=(2,2),data_format=self.data_format)(imgs)\n",
        "            mask_up = UpSampling2D(size=(2,2),data_format=self.data_format)(masks)\n",
        "            if self.data_format == 'channels_first' :\n",
        "                axis = 1\n",
        "            else :\n",
        "                axis = 3\n",
        "            \n",
        "            I_up_concat    = Concatenate(axis)([I_up,en_img])\n",
        "            mask_up_concat = Concatenate(axis)([mask_up,en_mask])\n",
        "            \n",
        "            I_out, Mask_out = Pconv(filters,kernel_size,padding='same')([I_up,mask_up])\n",
        "            if bn:\n",
        "                I_out = BatchNormalization()(I_out)\n",
        "            I_out = LeakyReLU(alpha=0.2)(I_out)\n",
        "            return I_out, Mask_out\n",
        "        \n",
        "        dec_conv1, dec_mask1 = decoder_layer(en_img8,en_mask8,en_img7,en_mask7,512,3)\n",
        "        dec_conv2, dec_mask2 = decoder_layer(dec_conv1,dec_mask1,en_img6,en_mask6,512,3)\n",
        "        dec_conv3, dec_mask3 = decoder_layer(dec_conv2,dec_mask2,en_img5,en_mask5,512,3)\n",
        "        dec_conv4, dec_mask4 = decoder_layer(dec_conv3,dec_mask3,en_img4,en_mask4,512,3)\n",
        "        dec_conv5, dec_mask5 = decoder_layer(dec_conv4,dec_mask4,en_img3,en_mask3,256,3)\n",
        "        dec_conv6, dec_mask6 = decoder_layer(dec_conv5,dec_mask5,en_img2,en_mask2,128,3)\n",
        "        dec_conv7, dec_mask7 = decoder_layer(dec_conv6,dec_mask6,en_img1,en_mask1,64,3)\n",
        "        dec_conv8, dec_mask8 = decoder_layer(dec_conv7,dec_mask7,inputs_img,inputs_mask,3,3,bn=False)\n",
        "        \n",
        "        outputs = Conv2D(3, 1, activation = 'sigmoid', name='outputs_img')(dec_conv8)\n",
        "        model = Model(inputs=[inputs_img, inputs_mask], outputs=outputs)\n",
        "\n",
        "        return model, inputs_mask \n",
        "                \n",
        "    \n",
        "    def model_compile(self,model,masks,lr=0.001):\n",
        "        model.compile(\n",
        "                     loss=self.loss_total(masks),\n",
        "                     optimizer= Adam(lr=lr),\n",
        "                     metrics=[self.PSNR]\n",
        "                     ) \n",
        "        \n",
        "    \n",
        "    def loss_total(self,M):\n",
        "        \n",
        "        def loss(I_out,I_gt):\n",
        "            '''I_comp is obtained by setting all the non hole pixels of I_out directly to ground truth'''\n",
        "            I_comp   = I_out*(1-M) + I_gt*M\n",
        "            vgg_out  = self.vgg(I_out)\n",
        "            vgg_comp = self.vgg(I_comp)\n",
        "            vgg_gt   = self.vgg(I_gt)\n",
        "        \n",
        "            l1 = self.hole_loss(I_out,I_gt,M)\n",
        "            l2 = self.loss_valid(I_out,I_gt,M)\n",
        "            l3 = self.loss_perceptual(vgg_out,vgg_gt,vgg_comp)\n",
        "            l4 = self.style_loss(vgg_out,vgg_gt)\n",
        "            l5 = self.style_loss(vgg_comp,vgg_gt)\n",
        "            l6 = self.loss_tv(M,I_comp)\n",
        "        \n",
        "            return l1 + 6*l2 + 0.05*l3 + 120*(l4+l5) + 0.1*l6\n",
        "        return loss\n",
        "        \n",
        "        \n",
        "    def hole_loss(self,I_out,I_gt,M):\n",
        "        '''Per pixel loss defined as the L1 distance between the ground truth and the output image \n",
        "           after multiplying by (1-M) where M is the corresponding mask'''\n",
        "        return self.l1((1-M)*I_out,(1-M)*I_gt)\n",
        "    \n",
        "    def loss_valid(self,I_out,I_gt,M):\n",
        "        ''' loss_valid = L1 distance of M * (I_out-I_gt)'''\n",
        "        return self.l1(M*I_out,M*I_gt)\n",
        "    \n",
        "    def loss_perceptual(self,vgg_out,vgg_gt,vgg_comp):\n",
        "        loss = 0\n",
        "        for i,j,k in zip(vgg_out,vgg_comp,vgg_gt) :\n",
        "            '''if K.ndim(i) == 4:\n",
        "                _,n1,n2,n3 = K.shape(i)\n",
        "            else :\n",
        "                n1,n2,n3 = K.shape(i)\n",
        "            n = n1*n2*n3'''\n",
        "            loss += self.l1(i,k) + self.l1(j,k)\n",
        "        return loss\n",
        "    \n",
        "    def style_loss(self,output,vgg_gt):\n",
        "        loss = 0\n",
        "        for o, g in zip(output, vgg_gt):\n",
        "            loss += self.l1(self.gram_matrix(o), self.gram_matrix(g))\n",
        "        return loss\n",
        "    \n",
        "    def loss_tv(self, mask, y_comp):\n",
        "        \"\"\"Total variation loss, used for smoothing the hole region, see. eq. 6\"\"\"\n",
        "\n",
        "        # Create dilated hole region using a 3x3 kernel of all 1s.\n",
        "        kernel = K.ones(shape=(3, 3, mask.shape[3], mask.shape[3]))\n",
        "        dilated_mask = K.conv2d(1-mask, kernel, data_format='channels_last', padding='same')\n",
        "\n",
        "        # Cast values to be [0., 1.], and compute dilated hole region of y_comp\n",
        "        dilated_mask = K.cast(K.greater(dilated_mask, 0), 'float32')\n",
        "        P = dilated_mask * y_comp\n",
        "\n",
        "        # Calculate total variation loss\n",
        "        a = self.l1(P[:,1:,:,:], P[:,:-1,:,:])\n",
        "        b = self.l1(P[:,:,1:,:], P[:,:,:-1,:])        \n",
        "        return a+b\n",
        "\n",
        "    def model_summary(self):\n",
        "        return self.model.summary()\n",
        "    \n",
        "    \n",
        "    def generator(self,generator,*args,**kwargs):\n",
        "        self.model.fit_generator(generator,*args,**kwargs)\n",
        "        \n",
        "    ##############################################################################\n",
        "    '''Gram-matrix function taken from github.\n",
        "       It is required for the style loss terms'''\n",
        "    \n",
        "    def gram_matrix(self,x, norm_by_channels=False):\n",
        "        \"\"\"Calculate gram matrix used in style loss\"\"\"\n",
        "        \n",
        "        # Assertions on input\n",
        "        assert K.ndim(x) == 4, 'Input tensor should be a 4d (B, H, W, C) tensor'\n",
        "        assert K.image_data_format() == 'channels_last', \"Please use channels-last format\"        \n",
        "        \n",
        "        # Permute channels and get resulting shape\n",
        "        x = K.permute_dimensions(x, (0, 3, 1, 2))\n",
        "        shape = K.shape(x)\n",
        "        B, C, H, W = shape[0], shape[1], shape[2], shape[3]\n",
        "        \n",
        "        # Reshape x and do batch dot product\n",
        "        features = K.reshape(x, K.stack([B, C, H*W]))\n",
        "        gram = K.batch_dot(features, features, axes=2)\n",
        "        \n",
        "        # Normalize with channels, height and width\n",
        "        gram = gram /  K.cast(C * H * W, x.dtype)\n",
        "        \n",
        "        return gram\n",
        "    ################################################################################################\n",
        "    \n",
        "    \n",
        "    def l1(self,y_true, y_pred):\n",
        "        \"\"\"Calculate the L1 loss used in all loss calculations\"\"\"\n",
        "        if K.ndim(y_true) == 4:\n",
        "            return K.mean(K.abs(y_pred - y_true), axis=[1,2,3])\n",
        "        elif K.ndim(y_true) == 3:\n",
        "            return K.mean(K.abs(y_pred - y_true), axis=[1,2])\n",
        "        else:\n",
        "            raise NotImplementedError(\"Calculating L1 loss on 1D tensors? should not occur for this network\")\n",
        "            \n",
        "    def PSNR(self,y_true, y_pred):\n",
        "        \"\"\"\n",
        "        PSNR is Peek Signal to Noise Ratio, see https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\n",
        "        The equation is:\n",
        "        PSNR = 20 * log10(MAX_I) - 10 * log10(MSE)\n",
        "        \n",
        "        Our input is scaled with be within the range -2.11 to 2.64 (imagenet value scaling). We use the difference between these\n",
        "        two values (4.75) as MAX_I        \n",
        "        \"\"\"        \n",
        "        #return 20 * K.log(4.75) / K.log(10.0) - 10.0 * K.log(K.mean(K.square(y_pred - y_true))) / K.log(10.0) \n",
        "        return - 10.0 * K.log(K.mean(K.square(y_pred - y_true))) / K.log(10.0) \n",
        "\n",
        "            \n",
        "    def predict(self,test,**kwargs):\n",
        "        return self.model.predict(test,**kwargs)\n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}